# TEAM 04 ONNX-MLIR
## Members
| Name | Student ID |
| ---- | ----- | 
| 郑扬  | PB21051134 |
| 张昊鹏 |PB21050991 |

## 实验选题
- 对基于mlir框架的深度学习编译器onnx-mlir进行生态调研和性能测评，并且分析onnx-mlir的执行逻辑，分析其中的优点和不足，提出相应的优化手段

## 设计
- 调查mlir框架和onnx模型格式，理解各自的执行逻辑和内容详情
- 深读onnx-mlir的论文和onnx-mlir项目，了解onnx-mlir的编译步骤、已经实现的优化和相应功能
- 使用onnx-mlir和mlir将.onnx模型编译为.so，并且使用对应的runtime运行时库加载模型和图片进行推理，将onnx-mlir和python中的onnx-runtime库加载同样的数据集和模型进行推理，对比两者之间的推理结果、占用资源和推理效率，分析其中的区别和原因
- 查看onnx-mlir的中间表示输出，同时编译特定功能的模型（比如只含有两个tensor相加的函数），分析onnx-mlir的输出，得到onnx-mlir推理效率低的原因，根据中间表示输出得到onnx-mlir并未进行的pass优化，提出相应的可能可以优化的点
- 尝试根据提出的优化写一种dialect和对应的pass接入mlir框架当中

## 具体实现
- 实现过程：训练了一个针对minist数据集的lenet模型，并将其导出为.onnx模型，使用onnx-mlir将lenet.onnx模型编译为lenet.onnx.mlir中间表示，将lenet.onnx.mlir使用mlir编译为lenet.so。使用Pyruntime.so运行时库加载lenet.so和minist数据集中的测试集，推理得到对应的结果，使用onnx-runtime加载lenet.onnx和minist数据集中的测试集，推理得到对应的结果，分析两者cpu占用率、推理结果和推理时间。使用onnx-mlir将lenet.onnx编译为lenet.onnx.ir，得到ONNX dialect和KRNL dialect每一层的中间表示，分析其中做了的优化和缺少的优化，提出相应可能进行的pass
- 评测结果：onnx-mlir推理结果和onnx-runtime的推理结果相同，推理时间onnx-mlir是onnx-runtime的16倍，而cpu占用率onnx-mlir是onnx-runtime的1/8，说明onnx-mlir并没有进行并行化的优化，而且根据ONNX dialect的中间表示可以知道onnx-mlir没有进行特定针对模型的优化，比如算子之间的融合这类优化并未实现

## 分工及贡献
- 郑扬：阅读onnx-mlir论文和onnx-mlir源码了解onnx-mlir的实现过程，训练了lenet.onnx模型，结合onnx-mlir和mlir实现lenet.onnx模型的编译，对比onnx-mlir和onnx-runtime的运行结果并分析两者的区别，根据onnx-mlir的中间表示输出总结了onnx-mlir缺少的优化并提出相应的优化手段
 
## 第一次提交
### 提交内容
- 郑扬：使用minist训练集训练了一个lenet.onnx，成功利用onnx-mlir和mlir实现lenet.onnx模型的编译并且在amd64 cpu机器上使用lenet.so推理minist测试集，对比python的onnx-runtime库加载lenet.onnx模型推理，对比两者得出区别，**对应SourceCode/onnx-mlir**

## 第二次提交
### 提交内容
- 郑扬：阅读onnx-mlir源码了解了onnx-mlir的实现逻辑，根据onnx-mlir的两个dialect实现提出了onnx-mlir可能没有进行优化的点，并编写了一个ADD.onnx和分析中间表示输出对自己的想法进行了验证，**对应SourceCode/dialect-output**

## 第三次提交
### 提交内容
- 郑扬：~~根据验证出没有优化的点尝试编写了新的dialect和pass~~，但是没有成功，根据了解的onnx-mlir运行机制、前面的实验结果等内容制作了PPT，**对应SourceCode/ppt**

## 相关的链接
- onnx-mlir论文:https://arxiv.org/abs/2008.08272
- onnx-mlir项目:https://github.com/onnx/onnx-mlir/tree/main